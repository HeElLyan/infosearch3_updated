# -*- coding: utf-8 -*-
"""3задание.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13gaA_8myEywozBlC_AmitMFjUT-sIwkM
"""

from google.colab import drive
drive.mount('gdrive')

default_path = 'gdrive/My Drive/4курс/Инфопоиск/result/'
input_path = 'gdrive/My Drive/4курс/Инфопоиск/result/text_outputs/'
output_inverted_path = 'gdrive/My Drive/4курс/Инфопоиск/result/inverted_index.txt'

import string
import codecs
from enum import unique
from itertools import chain
import re
import nltk
from nltk.corpus import stopwords

nltk.download('stopwords')
sw_nltk = stopwords.words('english')

def get_words_from_file(input):

    # with open(input) as source, open(output, 'w') as destination:
    with codecs.open(input) as source:
      
        #get the text from file
        text = source.read()
        #lower the text
        text = str(text.lower())

        text = re.sub(r'\w*\d\w*', '', text)

        symb = '.,!;()[]-:"/\|$@^''&*%?'
        for char in symb:
            text = text.replace(char, "")

        #split words in text
        words = text.split()
    
    for word in words: 
        if word == '':
            words.remove(word)

    #reduce stopwords
    words = [word for word in words if word not in sw_nltk]      

    return words

N = 142

# get all texts from site
all_texts = []

for i in range(N):

    input = input_path + 'выкачка' + str(i + 1) + '.txt'

    with open(input) as source:
        words = get_words_from_file(input)

    all_texts.append(words)

len(all_texts)

input_tokens_path = 'gdrive/My Drive/4курс/Инфопоиск/result/tokens.txt'
input_lemmas_path = 'gdrive/My Drive/4курс/Инфопоиск/result/lemmas.txt'

# get tokens
with open(input_tokens_path) as source:
    tokens = source.read()

tokens = tokens.split('\n')
del tokens[-1]

len(tokens)

# get lemmas
with open(input_lemmas_path) as source:
    lemmas = source.read()

lemmas = lemmas.split('\n')
del lemmas[-1]

len(lemmas)

keys = [lemmas[i].rstrip().split(":")[0] for i in range(len(lemmas))]
print(keys)

vals = []
for i in range(len(lemmas)):
    local_list = []
  
    default_lemma = lemmas[i].rstrip().replace(":", "").split(" ")

    for j in range(1, len(default_lemma)):
        local_res = default_lemma[j]
        local_list.append(local_res)

    vals.append(local_list)

print(vals)

len(keys)

tokens_inverted_indexes = tokens.copy()

#i - file number, j - position of token in i file
for k in range(len(tokens)):
    for i in range(len(all_texts)):
        count = 0
        for j in range(len(all_texts[i])):
            if all_texts[i][j] == tokens[k]:
                count += 1
        if count == 0:
            continue
        tokens_inverted_indexes[k] +=  ' ' + str(i + 1) + ':' + str(count)

print(tokens_inverted_indexes)

len(tokens_inverted_indexes)

lemmas_inverted_indexes = lemmas.copy()

#i - file number, j - position of token in i file
for k in range(len(vals)):
    for n in range(len(vals[k])):
      for i in range(len(all_texts)):
          count = 0
          for j in range(len(all_texts[i])):
              if all_texts[i][j] == vals[k][n]:
                  count += 1
          if count == 0:
              continue
          lemmas_inverted_indexes[k] +=  ' ' + str(i + 1) + ':' + str(count)

print(lemmas_inverted_indexes)

len(lemmas_inverted_indexes)

def write_data_to_file(output, data, name):
    with open(output + name + '.txt', 'w') as destination:
        for i in range(len(data)):
            destination.write(data[i] + '\n')

write_data_to_file(default_path, tokens_inverted_indexes, 'tokens_inverted_indexes_updated')
write_data_to_file(default_path, lemmas_inverted_indexes, 'lemmas_inverted_indexes_updated')